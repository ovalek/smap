{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UHK SMAP ovalek 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /usr/lib64/python3.10/site-packages (1.22.0)\n",
      "Requirement already satisfied: pandas in /home/ovalek/.local/lib/python3.10/site-packages (1.5.2)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "     |████████████████████████████████| 588.3 MB 51 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /home/ovalek/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3.10/site-packages (from pandas) (2022.6)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     |████████████████████████████████| 1.7 MB 9.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.7.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "     |████████████████████████████████| 4.5 MB 13.5 MB/s            \n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "     |████████████████████████████████| 77 kB 4.9 MB/s             \n",
      "\u001b[?25hCollecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
      "     |████████████████████████████████| 14.1 MB 8.6 MB/s            \n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "     |████████████████████████████████| 124 kB 10.2 MB/s            \n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     |████████████████████████████████| 57 kB 6.1 MB/s             \n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 8.5 MB/s            \n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.28.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "     |████████████████████████████████| 2.4 MB 17.2 MB/s            \n",
      "\u001b[?25hCollecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
      "     |████████████████████████████████| 6.0 MB 16.7 MB/s            \n",
      "\u001b[?25hCollecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     |████████████████████████████████| 439 kB 15.2 MB/s            \n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     |████████████████████████████████| 65 kB 5.2 MB/s             \n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.1.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "     |████████████████████████████████| 4.8 MB 15.9 MB/s            \n",
      "\u001b[?25hCollecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.38.4-py3-none-any.whl (36 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     |████████████████████████████████| 781 kB 12.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.27.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "     |████████████████████████████████| 93 kB 2.5 MB/s             \n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 12.7 MB/s            \n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.15.0-py2.py3-none-any.whl (177 kB)\n",
      "     |████████████████████████████████| 177 kB 11.2 MB/s            \n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "     |████████████████████████████████| 232 kB 16.3 MB/s            \n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3.10/site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     |████████████████████████████████| 155 kB 16.0 MB/s            \n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.11)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.12)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     |████████████████████████████████| 77 kB 7.2 MB/s             \n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     |████████████████████████████████| 151 kB 14.5 MB/s            \n",
      "\u001b[?25hInstalling collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, MarkupSafe, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.1 absl-py-1.3.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-22.12.6 gast-0.4.0 google-auth-2.15.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.1 h5py-3.7.0 keras-2.11.0 libclang-14.0.6 markdown-3.4.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.28.0 termcolor-2.1.1 typing-extensions-4.4.0 werkzeug-2.2.2 wheel-0.38.4 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "%pip install numpy pandas tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['small', 'parameters_products_color_sanitized.csv', 'parameters_products_pattern_sanitized.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "print(os.listdir(\"./data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 00:56:53.535623: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2088960000 exceeds 10% of free system memory.\n",
      "2022-12-14 00:56:53.748080: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2088960000 exceeds 10% of free system memory.\n",
      "2022-12-14 00:56:54.113711: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2088960000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 2048)              522242048 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               524544    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 48)                12336     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 522,778,928\n",
      "Trainable params: 522,778,928\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Initializes a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# First layer (pictures are 240x340=85000 x 3 colors => 255000)\n",
    "model.add(Dense(2048, activation='relu', input_shape=(255000, )))\n",
    "\n",
    "# Second layer\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# Output layer (multi-label classification therefore use sigmoid activation function)\n",
    "model.add(Dense(48, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-label classification therefore use binary_crossentropy\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are id_product, id_photo, date_update, position, show_in_lead\n",
      "Processed 16161 lines.\n",
      "Found products <method-wrapper '__len__' of list object at 0x7fc21c72ae40>.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "\n",
    "# load labels\n",
    "label_ids = []\n",
    "labels = []\n",
    "with open('./data/parameters_list_sanitized.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue\n",
    "        else:\n",
    "            line_count += 1\n",
    "            if not row[0] in label_ids:\n",
    "                label_ids += [row[0]]\n",
    "                labels += [row[2]]\n",
    "\n",
    "# load product label relations\n",
    "products_labels = {}\n",
    "with open('./data/parameters_products_color_sanitized.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue\n",
    "        else:\n",
    "            line_count += 1\n",
    "            if not row[0] in products_labels:\n",
    "                products_labels[row[0]] = []\n",
    "            for label_index, label_id in enumerate(label_ids):\n",
    "                if label_id == row[2]:\n",
    "                    products_labels[row[0]] += [labels[label_index]]\n",
    "with open('./data/parameters_products_pattern_sanitized.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue\n",
    "        else:\n",
    "            line_count += 1\n",
    "            if not row[0] in products_labels:\n",
    "                products_labels[row[0]] = []\n",
    "            for label_index, label_id in enumerate(label_ids):\n",
    "                if label_id == row[2]:\n",
    "                    products_labels[row[0]] += [labels[label_index]]\n",
    "\n",
    "# load products with main photo\n",
    "products = []\n",
    "with open('./data/photos_sanitized.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            line_count += 1\n",
    "            if row[4] != 'Y':\n",
    "                continue\n",
    "            found = False\n",
    "            for product in products:\n",
    "                if product[0] == row[0]:\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                products += [(row[0], row[1], row[3], row[4], ), ]\n",
    "    print(f'Processed {line_count} lines.')\n",
    "    print(f'Found products {products.__len__}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for dataset\n",
    "data_photos_paths = []\n",
    "data_labels = []\n",
    "for product in products:\n",
    "    data_photos_paths += [f'./data/small/{int(product[0]) % 10}/{product[0]}/{product[0]}_{product[1]}_{product[3]}_{product[2]}_small.webp']\n",
    "    tmp_product_labels = products_labels[product[0]]\n",
    "    tmp_labels_vector = []\n",
    "    for label in labels:\n",
    "        tmp_labels_vector += [1 if label in tmp_product_labels else 0]\n",
    "    data_labels += [tmp_labels_vector]\n",
    "\n",
    "data_photos_paths = tf.constant(data_photos_paths)\n",
    "data_labels = tf.constant(data_labels)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data_photos_paths, data_labels))\n",
    "\n",
    "def _prepare_picture_func(picturepath, labels):\n",
    "    image_string = tf.io.read_file(picturepath)\n",
    "    image = tf.image.decode_image(image_string, channels=3)\n",
    "    image.set_shape([None, None, None])\n",
    "    image = tf.image.resize(image, [150, 150])\n",
    "    image = tf.subtract(image, 116.779) # Zero-center by mean pixel\n",
    "    image.set_shape([150, 150, 3])\n",
    "    # image = tf.reverse(image, axis=[2]) # 'RGB'->'BGR'\n",
    "    # d = dict(zip([picturepath], [image])), labels\n",
    "    return (picturepath, image), labels\n",
    "    # return d\n",
    "dataset = dataset.map(_prepare_picture_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use NN for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# train_data = train_data[(train_labels >= 0) & (train_labels < 3)][0:50].reshape(-1, 28, 28, 1)\n",
    "# train_labels = train_labels[(train_labels >= 0) & (train_labels < 3)][0:50]\n",
    "# train_labels = pd.get_dummies(train_labels).to_numpy()\n",
    "\n",
    "# test_data = test_data[(test_labels >= 0) & (test_labels < 3)][0:10].reshape(-1, 28, 28, 1)\n",
    "# test_labels = test_labels[(test_labels >= 0) & (test_labels < 3)][0:10]\n",
    "# test_labels = pd.get_dummies(test_labels).to_numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
