{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UHK SMAP ovalek 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /usr/lib64/python3.10/site-packages (1.22.0)\n",
      "Requirement already satisfied: pandas in /home/ovalek/.local/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: tensorflow in /home/ovalek/.local/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow_datasets in /home/ovalek/.local/lib/python3.10/site-packages (4.7.0)\n",
      "Requirement already satisfied: scikit-learn in /home/ovalek/.local/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: scipy in /home/ovalek/.local/lib/python3.10/site-packages (1.9.3)\n",
      "Requirement already satisfied: matplotlib in /home/ovalek/.local/lib/python3.10/site-packages (3.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ovalek/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3.10/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (22.12.6)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.10/site-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (1.51.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (2.1.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3.10/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (0.28.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: toml in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /usr/lib/python3.10/site-packages (from tensorflow_datasets) (4.64.1)\n",
      "Requirement already satisfied: etils[epath] in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow_datasets) (0.9.0)\n",
      "Requirement already satisfied: dill in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow_datasets) (0.3.6)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow_datasets) (1.12.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3.10/site-packages (from tensorflow_datasets) (2.27.1)\n",
      "Requirement already satisfied: promise in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ovalek/.local/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ovalek/.local/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ovalek/.local/lib/python3.10/site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ovalek/.local/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3.10/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib64/python3.10/site-packages (from matplotlib) (9.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ovalek/.local/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ovalek/.local/lib/python3.10/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ovalek/.local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.11)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: zipp in /home/ovalek/.local/lib/python3.10/site-packages (from etils[epath]->tensorflow_datasets) (3.11.0)\n",
      "Requirement already satisfied: importlib_resources in /home/ovalek/.local/lib/python3.10/site-packages (from etils[epath]->tensorflow_datasets) (5.10.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /home/ovalek/.local/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.57.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ovalek/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ovalek/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ovalek/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ovalek/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ovalek/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ovalek/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ovalek/.local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "%pip install numpy pandas tensorflow tensorflow_datasets scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['small', 'photos_sanitized.csv', 'parameters_products_color_sanitized.csv', 'tmp_dataset', 'parameters_list_sanitized.csv', 'parameters_products_pattern_sanitized.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "print(os.listdir(\"./data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are id_product, id_photo, date_update, position, show_in_lead\n",
      "Processed 16161 lines.\n",
      "Found products <method-wrapper '__len__' of list object at 0x7f83cf3913c0>.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "\n",
    "# load labels\n",
    "label_ids = []\n",
    "label_names = []\n",
    "with open('./data/parameters_list_sanitized.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue\n",
    "        else:\n",
    "            line_count += 1\n",
    "            if not row[0] in label_ids:\n",
    "                label_ids += [row[0]]\n",
    "                label_names += [row[2]]\n",
    "\n",
    "# load product label relations\n",
    "products_labels = {}\n",
    "with open('./data/parameters_products_color_sanitized.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue\n",
    "        else:\n",
    "            line_count += 1\n",
    "            if not row[0] in products_labels:\n",
    "                products_labels[row[0]] = []\n",
    "            for label_index, label_id in enumerate(label_ids):\n",
    "                if label_id == row[2]:\n",
    "                    products_labels[row[0]] += [label_names[label_index]]\n",
    "with open('./data/parameters_products_pattern_sanitized.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue\n",
    "        else:\n",
    "            line_count += 1\n",
    "            if not row[0] in products_labels:\n",
    "                products_labels[row[0]] = []\n",
    "            for label_index, label_id in enumerate(label_ids):\n",
    "                if label_id == row[2]:\n",
    "                    products_labels[row[0]] += [label_names[label_index]]\n",
    "\n",
    "# load products with main photo\n",
    "products = []\n",
    "with open('./data/photos_sanitized.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            line_count += 1\n",
    "            if row[4] != 'Y':\n",
    "                continue\n",
    "            found = False\n",
    "            for product in products:\n",
    "                if product[0] == row[0]:\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                products += [(row[0], row[1], row[3], row[4], ), ]\n",
    "    print(f'Processed {line_count} lines.')\n",
    "    print(f'Found products {products.__len__}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "# prepare data for dataset\n",
    "data_photos_paths = []\n",
    "data_labels = []\n",
    "for product in products:\n",
    "    tmp_path = f'./data/small/{int(product[0]) % 10}/{product[0]}/{product[0]}_{product[1]}_{product[3]}_{product[2]}_small.webp'\n",
    "    if not exists(tmp_path):\n",
    "        continue\n",
    "    data_photos_paths += [tmp_path]\n",
    "    tmp_product_labels = products_labels[product[0]]\n",
    "    tmp_labels_vector = []\n",
    "    for label in label_names:\n",
    "        tmp_labels_vector += [1 if label in tmp_product_labels else 0]\n",
    "    data_labels += [tmp_labels_vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_photos_paths_train, data_photos_paths_tmp, data_labels_train, data_labels_tmp = train_test_split(data_photos_paths, data_labels, test_size=0.25, random_state=42)\n",
    "data_photos_paths_test, data_photos_paths_validation, data_labels_test, data_labels_validation = train_test_split(data_photos_paths_tmp, data_labels_tmp, test_size=0.4, random_state=None, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "def _create_dataset(X, y):\n",
    "    # X = tf.constant(X)\n",
    "    # y = tf.constant(y)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "\n",
    "    def _prepare_picture_func(picturepath, labels):\n",
    "        image_string = tf.io.read_file(picturepath)\n",
    "        image = tf.image.decode_image(image_string, channels=3)\n",
    "        image.set_shape([None, None, None])\n",
    "        image = tf.image.resize(image, [250, 340])\n",
    "        # image = tf.subtract(image, 116.779) # Zero-center by mean pixel\n",
    "        # # image.set_shape([200, 200, 3])\n",
    "        # image.set_shape((250, 250, 3))\n",
    "        image.set_shape((250, 340, 3))\n",
    "        # image = image.reshape(-1, 250, 340, 3)\n",
    "        # image = tf.reverse(image, axis=[2]) # 'RGB'->'BGR'\n",
    "        # d = dict(zip([picturepath], [image])), labels\n",
    "        # return (picturepath, image), labels\n",
    "        return image, labels\n",
    "        # return d\n",
    "    return dataset.map(_prepare_picture_func)\n",
    "\n",
    "train_ds = _create_dataset(data_photos_paths_train, data_labels_train)\n",
    "test_ds = _create_dataset(data_photos_paths_test, data_labels_test)\n",
    "validation_ds = _create_dataset(data_photos_paths_validation, data_labels_validation)\n",
    "\n",
    "\n",
    "# data_photos_paths = tf.constant(data_photos_paths)\n",
    "# data_labels = tf.constant(data_labels)\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((data_photos_paths, data_labels))\n",
    "\n",
    "# def _prepare_picture_func(picturepath, labels):\n",
    "#     image_string = tf.io.read_file(picturepath)\n",
    "#     image = tf.image.decode_image(image_string, channels=3)\n",
    "#     image.set_shape([None, None, None])\n",
    "#     image = tf.image.resize(image, [150, 150])\n",
    "#     image = tf.subtract(image, 116.779) # Zero-center by mean pixel\n",
    "#     image.set_shape([150, 150, 3])\n",
    "#     # image = tf.reverse(image, axis=[2]) # 'RGB'->'BGR'\n",
    "#     # d = dict(zip([picturepath], [image])), labels\n",
    "#     return (picturepath, image), labels\n",
    "#     # return d\n",
    "# dataset = dataset.map(_prepare_picture_func)\n",
    "\n",
    "\n",
    "\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "# dataset = dataset.shuffle(buffer_size=100, seed=19575364)\n",
    "# dataset.save('./data/tmp_dataset')\n",
    "\n",
    "\n",
    "\n",
    "# train_set, test_set, valid_set = tfds.load('./data/tmp_dataset', \n",
    "#                                            split=[\"train[0%:75%]\", \"test[75%:90%]\", \"valid[90%:100%]\"],\n",
    "#                                            as_supervised=True, with_info=True, download=False)\n",
    "\n",
    "#batch = dataset.batch(20)\n",
    "\n",
    "\n",
    "# train_set, tmp_set = tf.keras.utils.split_dataset(\n",
    "#     dataset, left_size=0.75, right_size=0.25, shuffle=True, seed=19575364\n",
    "# )\n",
    "# test_set, valid_set = tf.keras.utils.split_dataset(\n",
    "#     dataset, left_size=0.60, right_size=0.40, shuffle=False, seed=None\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch = train_ds.batch(32)\n",
    "# test_batch = test_ds.batch(32)\n",
    "\n",
    "# # check batch\n",
    "# for image_batch, labels_batch in train_batch:\n",
    "#   print(image_batch.shape)\n",
    "#   print(labels_batch.shape)\n",
    "#   break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "\n",
    "# # Initializes a sequential model\n",
    "# model = Sequential()\n",
    "\n",
    "# # First layer (pictures are 240x340=85000 x 3 colors => 255000)\n",
    "# # model.add(Dense(2048, activation='relu', input_shape=(255000, )))\n",
    "# model.add(Dense(2048, activation='relu', input_shape=(250, 250, 3)))\n",
    "# # model.add(Dense(2048, activation='relu', input_shape=(187500, )))\n",
    "# # model.add(Dense(2048, activation='relu', input_dim=187500))\n",
    "\n",
    "# # Second layer\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# # Output layer (multi-label classification therefore use sigmoid activation function)\n",
    "# model.add(Dense(48, activation='sigmoid'))\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "num_classes = len(label_names)\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Rescaling(1./255, input_shape=(250, 340, 3)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='sigmoid'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_5 (Rescaling)     (None, 250, 340, 3)       0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 250, 340, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 125, 170, 16)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 125, 170, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 62, 85, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 62, 85, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 31, 42, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 83328)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               10666112  \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 48)                6192      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,695,888\n",
      "Trainable params: 10,695,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# multi-label classification therefore use binary_crossentropy\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'], run_eagerly=True)\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.binary_crossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use NN for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PIL.Image.open(str(data_photos_paths[58]))\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# @tf.function\n",
    "# def _preview():\n",
    "#   plt.figure(figsize=(10, 10))\n",
    "#   for images, labels in train_ds.take(1):\n",
    "#     for i in range(9):\n",
    "#       ax = plt.subplot(3, 3, i + 1)\n",
    "#       # plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#       # plt.imshow(images[i][1])\n",
    "#       tmp_title = []\n",
    "#       for label_index in labels:\n",
    "#         tf.gather(list, tf_look_up[index])\n",
    "#         tmp_title += [label_names[label_index]]\n",
    "#       plt.title(', '.join(tmp_title))\n",
    "#       plt.axis(\"off\")\n",
    "# _preview()\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# @tf.function\n",
    "# def preview():\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     for images, labels in train_ds.take(1):\n",
    "#         for i in range(9):\n",
    "#             ax = plt.subplot(3, 3, i + 1)\n",
    "#             plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#             plt.title(labels[labels[i]])\n",
    "#             plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  4/181 [..............................] - ETA: 2:12 - loss: 0.5254 - accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input is empty.\n\t [[{{node decode_image/DecodeImage}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 24\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# model.fit_generator(\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m#         train_batch, \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m#         steps_per_epoch=len(train_batch), # batch_size\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m#   epochs=epochs\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m     23\u001b[0m epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\n\u001b[0;32m---> 24\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     25\u001b[0m   train_ds\u001b[39m.\u001b[39;49mbatch(\u001b[39m32\u001b[39;49m),\n\u001b[1;32m     26\u001b[0m   validation_data\u001b[39m=\u001b[39;49mvalidation_ds\u001b[39m.\u001b[39;49mbatch(\u001b[39m32\u001b[39;49m),\n\u001b[1;32m     27\u001b[0m   epochs\u001b[39m=\u001b[39;49mepochs\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input is empty.\n\t [[{{node decode_image/DecodeImage}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint('model.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]\n",
    "\n",
    "# model.fit_generator(\n",
    "#         train_batch, \n",
    "#         steps_per_epoch=len(train_batch), # batch_size\n",
    "#         validation_data=test_batch,\n",
    "#         validation_steps=len(test_batch), # batch_size\n",
    "#         epochs=10,\n",
    "#         callbacks=callbacks\n",
    "#     )\n",
    "# epochs=10\n",
    "# history = model.fit(\n",
    "#   train_ds,\n",
    "#   validation_data=validation_ds,\n",
    "#   epochs=epochs\n",
    "# )\n",
    "epochs=10\n",
    "history = model.fit(\n",
    "  train_ds.batch(32),\n",
    "  validation_data=validation_ds.batch(32),\n",
    "  epochs=epochs\n",
    ")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
